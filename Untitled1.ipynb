{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYEgc4gjkA+0sb+jaMCr8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rfantinel/GAN/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppI93GQUkcbp"
      },
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "import models\n",
        "import models.sngan_64 as sngan_64\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import cv2\n",
        "\n",
        "\n",
        "from aux import save_imgs, load_save_state, to_np, plot_graph, gen_folder, cropping\n",
        "\n",
        "# torch.backends.cudnn.enabled = True\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda:0')\n",
        "        \n",
        "        self.perc_fault = 13\n",
        "        \n",
        "        self.channels = 3\n",
        "        self.img_size = 64\n",
        "        \n",
        "        # N CROPS\n",
        "        self.n_crops = 4\n",
        "\n",
        "        #_________ exp 0\n",
        "        # self.kC = 10\n",
        "        # self.kCC = 10\n",
        "        #_________ exp 1\n",
        "        self.kC = 1\n",
        "        self.kCC = 1\n",
        "\n",
        "        # batch size\n",
        "        self.dis_batch_size = 32\n",
        "        self.eval_batch_size = 32\n",
        "        self.gen_batch_size = 32\n",
        "\n",
        "        #  learning parameters\n",
        "        self.d_lr = 0.0002\n",
        "        self.g_lr = 0.0002\n",
        "        self.lr_decay = False\n",
        "        self.beta1 = 0.0\n",
        "        self.beta2 = 0.9\n",
        "        self.bottom_width = 4\n",
        "        \n",
        "        # models\n",
        "        self.latent_dim = 128\n",
        "        # self.df_dim = 128\n",
        "        self.df_dim = 32\n",
        "        # self.gf_dim = 256\n",
        "        self.gf_dim = 64\n",
        "\n",
        "        \n",
        "\n",
        "        self.d_spectral_norm = True\n",
        "        self.data_path = './data'\n",
        "        self.dataset = 'cifar10'\n",
        "\n",
        "\n",
        "        self.exp_name = 'sngan_64'\n",
        "        self.g_spectral_norm = False\n",
        "\n",
        "        self.init_type = 'xavier_uniform'\n",
        "        self.load_path = None\n",
        "        self.max_epoch = 200\n",
        "        self.max_iter = 500000\n",
        "        # self.n_critic = 5\n",
        "        self.n_critic = 1\n",
        "        self.num_eval_imgs = 50000\n",
        "        self.num_workers = 1\n",
        "        self.print_freq = 50\n",
        "        self.random_seed = 12345\n",
        "        self.val_freq = 20\n",
        "\n",
        "        self.path_folder = './save_result_ufGAN'\n",
        "        self.path_img_folder, self.path_lastModel_folder, self.path_log_file, self.path_loss_img_file, self.path_acc_img_file = gen_folder(self.path_folder)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyCorrelationLoss(nn.Module):\n",
        "    def __init__(self, upperbound=1, slope=1):\n",
        "        super(MyCorrelationLoss, self).__init__()\n",
        "        self.M = upperbound\n",
        "        self.S = slope\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2, fg):\n",
        "        # y in [-1, 1]\n",
        "        x1_ = x1.view(x1.shape[0], -1)*0.5+0.5\n",
        "        x2_ = x2.view(x2.shape[0], -1)*0.5+0.5\n",
        "\n",
        "        eg = torch.mean(torch.abs(x1_ - x2_)**2, 1)\n",
        "        fg = fg.view(eg.shape)\n",
        "\n",
        "        # lg = fg*torch.exp(-eg/self.S)\n",
        "        lg =  (-fg+1)/2*self.M + fg*torch.min(fg*0+self.M, self.M*torch.exp(-eg/self.S))\n",
        "\n",
        "\n",
        "        loss = torch.mean(lg)\n",
        "                \n",
        "        return loss\n",
        "\n",
        "\n",
        "class LinearLrDecay(object):\n",
        "    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n",
        "\n",
        "        assert start_lr > end_lr\n",
        "        self.optimizer = optimizer\n",
        "        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n",
        "        self.decay_start_step = decay_start_step\n",
        "        self.decay_end_step = decay_end_step\n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "\n",
        "    def step(self, current_step):\n",
        "        if current_step <= self.decay_start_step:\n",
        "            lr = self.start_lr\n",
        "        elif current_step >= self.decay_end_step:\n",
        "            lr = self.end_lr\n",
        "        else:\n",
        "            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "\n",
        "def load_params(model, new_param):\n",
        "    for p, new_p in zip(model.parameters(), new_param):\n",
        "        p.data.copy_(new_p)\n",
        "\n",
        "\n",
        "def copy_params(model):\n",
        "    flatten = deepcopy(list(p.data for p in model.parameters()))\n",
        "    return flatten\n",
        "\n",
        "\n",
        "# weight init\n",
        "def weights_init(m):\n",
        "    # args = cfg.parse_args()\n",
        "    args = Args()\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv2d') != -1:\n",
        "        if args.init_type == 'normal':\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        elif args.init_type == 'orth':\n",
        "            nn.init.orthogonal_(m.weight.data)\n",
        "        elif args.init_type == 'xavier_uniform':\n",
        "            nn.init.xavier_uniform(m.weight.data, 1.)\n",
        "        else:\n",
        "            raise NotImplementedError('{} unknown inital type'.format(args.init_type))\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def add_fault(x, N):\n",
        "\n",
        "    idx = (np.random.rand(N)*x.shape[0]).astype(int)\n",
        "\n",
        "    for i in idx:\n",
        "        I = x[i]\n",
        "        I = I.numpy()\n",
        "        I = np.swapaxes(np.swapaxes(I, 0, 1), 1, 2)\n",
        "        \n",
        "        if I.dtype != 'uint8':\n",
        "            I = np.uint8(255*(I+1)/2)\n",
        "\n",
        "        h, w, c = I.shape\n",
        "\n",
        "        c = (np.random.rand(2)*np.array([h,w])).astype(int)\n",
        "        # d = np.random.rand(1)*h/3+h/5\n",
        "        d = 30\n",
        "        r = d/2\n",
        "        n = (np.random.rand(1)*6 + 3).astype(int)\n",
        "        a = np.random.rand(1)*np.pi*2\n",
        "        b = np.pi*2/n\n",
        "        ang = np.arange(start=a, stop=a+np.pi*2, step=b)%(np.pi*2)\n",
        "        pts = np.int32(np.transpose(np.vstack((c[0] + r*np.cos(ang), c[1]+r*np.sin(ang)))))\n",
        "\n",
        "        # cv2.imshow('I', I)\n",
        "\n",
        "        # cv2.fillPoly(I,[pts],(0, 255 , 255))\n",
        "        I = cv2.UMat.get(cv2.fillConvexPoly(I, pts, (255, 255, 0)))\n",
        "        \n",
        "        # cv2.imshow('I2', I)\n",
        "        # cv2.waitKey(0)\n",
        "\n",
        "\n",
        "        I = np.swapaxes(np.swapaxes(I, 2, 1), 1, 0)\n",
        "\n",
        "\n",
        "        x[i] = torch.from_numpy(I/255.0*2 - 1)\n",
        "\n",
        "\n",
        "    return x\n",
        "    \n",
        "\n",
        "class UFD_GAN():\n",
        "    def __init__(self, args, mode='WGAN'):\n",
        "        self.args = args\n",
        "        # set up discriminator \n",
        "        self.netD = sngan_64.Discriminator(args=self.args).to(self.args.device)\n",
        "        self.optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, self.netD.parameters()),\n",
        "                                     self.args.d_lr, (self.args.beta1, self.args.beta2))\n",
        "        self.schedulerD = LinearLrDecay(self.optimizerD, self.args.d_lr, 0.0, 0, self.args.max_iter * self.args.n_critic)\n",
        "        load_save_state(self.args.path_lastModel_folder, 'D', self.netD, self.optimizerD, self.schedulerD, 'load')\n",
        "\n",
        "        \n",
        "        # set up classifier\n",
        "        self.netC = sngan_64.Classifier(n_channels = self.args.channels).to(self.args.device)\n",
        "        self.optimizerC = optim.Adam(self.netC.parameters(), lr = self.args.d_lr/100, betas=(0.5, 0.999))\n",
        "        self.schedulerC = optim.lr_scheduler.ExponentialLR(self.optimizerC, gamma = 50)\n",
        "        load_save_state(self.args.path_lastModel_folder, 'C', self.netC, self.optimizerC, self.schedulerC, 'load')\n",
        "\n",
        "        self.ither_val = 0\n",
        "\n",
        "        # set up generator fast\n",
        "        self.netG = sngan_64.Generator(args=self.args).to(self.args.device)\n",
        "        self.optimizerG = torch.optim.Adam(filter(lambda p: p.requires_grad, self.netG.parameters()),\n",
        "                                     self.args.g_lr, (self.args.beta1, self.args.beta2))\n",
        "        self.schedulerG = LinearLrDecay(self.optimizerG, self.args.g_lr, 0.0, 0, self.args.max_iter * self.args.n_critic)\n",
        "        load_save_state(self.args.path_lastModel_folder, 'G', self.netG, self.optimizerG, self.schedulerG, 'load')\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self, epochs, dataloader, save_interval=50):\n",
        "        \n",
        "        schedulers = (self.schedulerG, self.schedulerD) if self.args.lr_decay else None\n",
        "        global_steps = 0\n",
        "        iter_val = 0\n",
        "\n",
        "        gen_avg_param = copy_params(self.netG)\n",
        "\n",
        "        loss_Dreal_iter = np.zeros([])\n",
        "        loss_Dfake_iter = np.zeros([])\n",
        "        loss_G_iter = np.zeros([])\n",
        "        loss_CC_iter = np.zeros([])\n",
        "        loss_C_iter = np.zeros([])\n",
        "\n",
        "        g_loss = 0\n",
        "        c_loss = 0\n",
        "        cc_loss = 0\n",
        "        rec0 = 0\n",
        "        rec1 = 0\n",
        "        acc_on_image = 0\n",
        "\n",
        "        for epoch in range(int(0), int(epochs)):\n",
        "          \n",
        "            gen_step = 0\n",
        "\n",
        "            # train mode\n",
        "            self.netG = self.netG.train()\n",
        "            self.netD = self.netD.train()\n",
        "\n",
        "            for iter_idx, (imgs, _) in enumerate(dataloader):\n",
        "                \n",
        "                bat_size = imgs.shape[0]\n",
        "\n",
        "                imgs = add_fault(imgs, (np.floor(bat_size*self.args.perc_fault/100).astype(int))\n",
        "                \n",
        "                # Adversarial ground truths\n",
        "                real_imgs = imgs.type(torch.cuda.FloatTensor)\n",
        "\n",
        "                # Sample noise as generator input               \n",
        "                z = torch.randn(bat_size, self.args.latent_dim + self.args.n_crops, 1, 1, device=self.args.device)\n",
        "                z[:, self.args.latent_dim:, 0, 0] = 0\n",
        "                idx_fault = torch.randint(0, self.args.n_crops, (int(bat_size*self.args.perc_fault/100),))\n",
        "                for i_f in range(int(bat_size*self.args.perc_fault/100)):\n",
        "                    z[i_f, int(self.args.latent_dim + idx_fault[i_f]), 0, 0] = 1\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "                self.optimizerD.zero_grad()\n",
        "\n",
        "                real_validity = self.netD(real_imgs)\n",
        "                fake_imgs = self.netG(z).detach()\n",
        "\n",
        "                fake_validity = self.netD(fake_imgs)            \n",
        "                \n",
        "                # cal loss\n",
        "                d_loss_real = torch.mean(nn.ReLU(inplace=True)(1.0 - real_validity))\n",
        "                d_loss_fake = torch.mean(nn.ReLU(inplace=True)(1 + fake_validity))\n",
        "                d_loss = d_loss_real + d_loss_fake\n",
        "                d_loss.backward()\n",
        "                self.optimizerD.step()\n",
        "\n",
        "\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "                if global_steps % self.args.n_critic == 0:\n",
        "\n",
        "                        \n",
        "                    # ---------------------\n",
        "                    #  Crop Compariso Loss\n",
        "                    # ---------------------\n",
        "                    # G0 \n",
        "                    input_G0 = torch.randn(int(bat_size/self.args.n_crops), self.args.latent_dim + self.args.n_crops, 1, 1, device=self.args.device)\n",
        "                    input_G0[:, self.args.latent_dim:, 0, 0] = 0\n",
        "                    fake_img_G0 = self.netG(input_G0)\n",
        "\n",
        "                    # G1\n",
        "                    input_G1 = input_G0.clone()\n",
        "                    idx_fault = torch.randint(0, self.args.n_crops, (input_G1.shape[0],))\n",
        "                    for i_f in range(idx_fault.shape[0]):\n",
        "                        input_G1[i_f, int(self.args.latent_dim + idx_fault[i_f]), 0, 0] = 1\n",
        "                    fake_img_G1 = self.netG(input_G1)\n",
        "                    \n",
        "\n",
        "                    fake_label = input_G1[:, self.args.latent_dim:].view(input_G1.shape[0], 1, int(self.args.n_crops**0.5), int(self.args.n_crops**0.5)).clone()\n",
        "\n",
        "                  \n",
        "                    # cropping\n",
        "                    crops_G0 = cropping(fake_img_G0, self.args.n_crops, self.args.device)\n",
        "                    crops_G1 = cropping(fake_img_G1, self.args.n_crops, self.args.device)\n",
        "                    y_C = cropping(fake_label, self.args.n_crops, self.args.device)\n",
        "\n",
        "                    y_C1 = y_C.view(-1, self.args.n_crops)\n",
        "                    idxLabel1 = torch.where(y_C1 == 1)\n",
        "                    idxLabel1 = idxLabel1[0]* self.args.n_crops + idxLabel1[1]\n",
        "\n",
        "                    idxLabel0 = torch.where(y_C1 == 0)\n",
        "                    x = torch.randint(low=0, high=self.args.n_crops-1, size=(y_C1.shape[0],) )\n",
        "                    y = x + torch.arange(0, idxLabel0[0].shape[0], self.args.n_crops-1)\n",
        "                    idxLabel0 = idxLabel0[0][y]*self.args.n_crops + idxLabel0[1][y]\n",
        "\n",
        "\n",
        "                    # crop comparison loss\n",
        "                    Xcomp0 = torch.cat((crops_G1[idxLabel0], crops_G0[idxLabel1], crops_G0[idxLabel0]), 0)\n",
        "                    Xcomp1 = torch.cat((crops_G1[idxLabel1], crops_G1[idxLabel1], crops_G1[idxLabel0]), 0)\n",
        "\n",
        "                    labelComp = torch.cat((y_C[idxLabel1], y_C[idxLabel1], y_C[idxLabel0]), 0) *2-1\n",
        "\n",
        "\n",
        "                    cc_loss = MyCorrelationLoss(upperbound=1, slope=1)(Xcomp0, Xcomp1, labelComp)\n",
        "                    \n",
        "\n",
        "                \n",
        "                    # -----------------\n",
        "                    #  Train Classifier\n",
        "                    # -----------------\n",
        "                    #                  \n",
        "\n",
        "                    self.optimizerC.zero_grad()\n",
        "                    \n",
        "                    in_C_l0 = torch.cat((crops_G0[idxLabel0], crops_G1[idxLabel0], crops_G0[idxLabel1]), 0)\n",
        "                    labelC_l0 = torch.cat((y_C[idxLabel0], y_C[idxLabel0], y_C[idxLabel0]), 0)\n",
        "\n",
        "                    in_C_l1 = crops_G1[idxLabel1]\n",
        "                    labelC_l1 = y_C[idxLabel1]\n",
        "\n",
        "                    out_C_fake_l0 = self.netC(in_C_l0)\n",
        "                    loss_C_fake_l0_1 = nn.BCELoss()(out_C_fake_l0, labelC_l0)\n",
        "\n",
        "                    out_C_fake_l1 = self.netC(in_C_l1)\n",
        "                    loss_C_fake_l1_1 = nn.BCELoss()(out_C_fake_l1, labelC_l1)\n",
        "\n",
        "                    c_loss = 0.33*loss_C_fake_l0_1 + 0.66*loss_C_fake_l1_1\n",
        "\n",
        "                    if global_steps > 50000:\n",
        "                        c_loss.backward(retain_graph=True) \n",
        "\n",
        "\n",
        "                    \n",
        "                    self.optimizerG.zero_grad()\n",
        "\n",
        "                    gen_z = torch.randn(bat_size, self.args.latent_dim + self.args.n_crops, 1, 1, device=self.args.device)\n",
        "                    gen_z[:, self.args.latent_dim:, 0, 0] = 0\n",
        "                    idx_fault = torch.randint(0, self.args.n_crops, (int(bat_size*self.args.perc_fault/100),))\n",
        "                    for i_f in range(int(bat_size*self.args.perc_fault/100)):\n",
        "                        gen_z[i_f, int(self.args.latent_dim + idx_fault[i_f]), 0, 0] = 1\n",
        "\n",
        "                    gen_imgs = self.netG(gen_z)\n",
        "                    fake_validity = self.netD(gen_imgs)\n",
        "\n",
        "                    # cal loss\n",
        "                    \n",
        "                    g_loss = -torch.mean(fake_validity) + self.args.kCC*cc_loss +  self.args.kC*c_loss\n",
        "                    \n",
        "                    g_loss.backward()\n",
        "\n",
        "                    if global_steps>50000:\n",
        "                      self.optimizerC.step()\n",
        "\n",
        "                    self.optimizerG.step()\n",
        "\n",
        "                    # adjust learning rate\n",
        "                    if schedulers:\n",
        "                        gen_scheduler, dis_scheduler = schedulers\n",
        "                        g_lr = gen_scheduler.step(global_steps)\n",
        "                        d_lr = dis_scheduler.step(global_steps)\n",
        "                        \n",
        "\n",
        "                    # moving average weight\n",
        "                    for p, avg_p in zip(self.netG.parameters(), gen_avg_param):\n",
        "                        avg_p.mul_(0.999).add_(0.001, p.data)\n",
        "\n",
        "                    gen_step += 1\n",
        "\n",
        "\n",
        "                # if global_steps > iter_val:\n",
        "                #     acc_on_image, rec0, rec1 = evaluate_model(self.netC1, dataloader_val, device, want_cropping=True, n_crops=N_CROPS)\n",
        "                \n",
        "\n",
        "\n",
        "                # Plot the progress\n",
        "                out_log = '[%d][%d/%d][%d/%d] Loss_D real: %.4f Loss_D fake: %.4f loss_G: %.4f loss_C : %.4f loss_CC : %.4f rec0 : %.4f rec1 : %.4f acc : %.4f' % (global_steps, epoch, epochs, iter_idx, len(dataloader), d_loss_real, d_loss_fake, g_loss, c_loss, cc_loss, rec0, rec1, acc_on_image)\n",
        "\n",
        "                print(out_log)\n",
        "\n",
        "               \n",
        "\n",
        "                loss_Dreal_iter = np.hstack((loss_Dreal_iter, to_np(d_loss_real) ))\n",
        "                loss_Dfake_iter = np.hstack((loss_Dfake_iter, to_np(d_loss_fake) ))\n",
        "                loss_G_iter     = np.hstack((loss_G_iter    , to_np(g_loss) ))\n",
        "                loss_CC_iter    = np.hstack((loss_CC_iter    , to_np(cc_loss) ))\n",
        "                loss_C_iter    = np.hstack((loss_C_iter     , to_np(c_loss) ))\n",
        "\n",
        "\n",
        "                # acc_x_image_ither = np.hstack((acc_x_image_ither, to_np(acc_on_image)))\n",
        "                # rec0_ither = np.hstack((rec0_ither, to_np(rec0)))\n",
        "                # rec1_ither = np.hstack((rec1_ither, to_np(rec1)))\n",
        "                \n",
        "                \n",
        "\n",
        "                if (global_steps % save_interval == 0):\n",
        "                    \n",
        "                    # plot graph loss\n",
        "                    plot_graph([loss_Dreal_iter, loss_Dfake_iter, loss_G_iter, loss_C_iter, loss_CC_iter], ['loss D real', 'loss D fake', 'loss G', 'loss C', 'loss CC'], self.args.path_loss_img_file, True)\n",
        "                   \n",
        "                    # # plot graph acc\n",
        "                    # plot_graph([acc_x_image_ither, rec0_ither, rec1_ither], ['acc', 'rec 0', 'rec 1'], path_acc_img_file)\n",
        "\n",
        "\n",
        "                    r, c = 2, 5               \n",
        "                    \n",
        "                    z = torch.randn(r*c, self.args.latent_dim + self.args.n_crops, 1, 1, device=self.args.device)\n",
        "                    z[:, self.args.latent_dim:, 0, 0] = 0\n",
        "                    idx_fault = torch.arange(self.args.n_crops).repeat(int(r*c/2/self.args.n_crops)+1)[0:int(r*c/2)]\n",
        "                    for i_f in range(int(r*c/2)):\n",
        "                        z[i_f, int(self.args.latent_dim + idx_fault[i_f]), 0, 0] = 1\n",
        "\n",
        "                    gen_imgs = 0.5 * self.netG(z) + 0.5   \n",
        "                    gen_imgs[gen_imgs<0] = 0\n",
        "                    gen_imgs[gen_imgs>1] = 1\n",
        "                    \n",
        "                    save_imgs( os.path.join( self.args.path_img_folder, 'img_generated_' + str(global_steps) ), gen_imgs, r, c, normalize=False )\n",
        "\n",
        "                                # do checkpointing\n",
        "                    torch.save(self.netG.state_dict(), os.path.join(self.args.path_lastModel_folder,'last_G_model.pth') )\n",
        "                    torch.save(self.optimizerG.state_dict(), os.path.join(self.args.path_lastModel_folder,'last_G_opt.pth') )\n",
        "                    \n",
        "                    torch.save(self.netD.state_dict(), os.path.join(self.args.path_lastModel_folder,'last_D_model.pth') )  \n",
        "                    torch.save(self.optimizerD.state_dict(), os.path.join(self.args.path_lastModel_folder,'last_D_opt.pth') ) \n",
        "\n",
        "\n",
        "                global_steps += 1\n",
        "    \n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # args = cfg.parse_args()\n",
        "    args = Args()\n",
        "\n",
        "    torch.cuda.manual_seed(args.random_seed)\n",
        "\n",
        "    ufd_gan = UFD_GAN(args=args)\n",
        "\n",
        "    # set up data_loader\n",
        "    # dataset = datasets.ImageDataset(args)\n",
        "    # train_loader = dataset.train\n",
        "\n",
        "\n",
        "    transform_data = transforms.Compose([transforms.Resize(size=(64,64)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    dataset = dset.CIFAR10(root='./data', download=True, transform=transform_data)\n",
        "   \n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.gen_batch_size, shuffle=True, num_workers=int(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    ufd_gan.train_model(epochs=200, dataloader=train_loader,save_interval=100 )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}